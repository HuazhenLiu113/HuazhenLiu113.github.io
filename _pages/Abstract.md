<span class='anchor' id='abstract'></span>

# High-Accuracy Phase Unwrapping Based on Binarized Wrap Count
Spatial phase unwrapping is essential for converting wrapped phase fringes into a continuous unwrapped phase map, which is critical for various high-precision measurement technologies. The accuracy of phase unwrapping directly affects measurement precision. Recently, deep learning-based phase unwrapping has emerged as a promising alternative to traditional methods, primarily due to its strong resilience against noise. However, existing approaches often struggle to produce consistent results, limiting their practical applicability. This study introduces Binarized Wrap Count Phase Unwrapping (BWCPU), a novel method that utilizes neural networks to analyze phase gradient structures through binarized wrap counts. This approach reduces prediction complexity while ensuring accurate phase segmentation. In structured light surface measurements, BWCPU significantly decreases misinterpretations in noisy conditions, achieving a remarkable 76.9% improvement over leading deep learning-based wrap-count estimation methods. Furthermore, by employing a stitching algorithm known as unidirectional optimal seam stitching, BWCPU extends its capabilities to handle 1024×1024 patterns, showcasing its potential for high-precision measurements in noisy environments.

# Motion-induced phase shift for dynamic structured light measurement
Structured light 3D shape measurement is extensively utilized in semiconductor inspection, smart manufacturing, and biomedical imaging due to its rapid measurement speed, high precision, and versatile applicability to different objects. However, the traditional implementations of this method often require that the object remains static while recording the phase-shifting structured light images, which limits the adaptability of dynamic measurement. Here, we propose a dynamic 3D shape measurement using structured light based on motion-induced phase shift (MIPS). As the object moves, the surface features distort the fringe pattern, resulting in a phase shifting effect. By employing the MIPS method, we can determine the phase even in the situations where the knowledge of phase shifting conditions is not accurate. This enables the acquisition of the 3D topography of the object surface with a high level of precision. Experimental results demonstrate that the MIPS method can accurately measure the 3D shape of objects moving as fast as 100 mm/s, with a relative discrepancy of less than 0.23%.

# High frame-rate reconfigurable diffractive neural network based on superpixels
The existing implementations of reconfigurable diffractive neural network rely on both a liquid crystal spatial light modulator and a digital micromirror device, which results in the complexity in the alignment of optical system and constrained computational speed. Here, we propose a superpixel diffractive neural network that leverages solely a digital micromirror device to control neuron bias and connection. This approach considerably simplifies the optical system and achieves an computational speed of 326Hz per neural layer. We validate our method through experiments in digit classification, achieving an accuracy of 82.6%, and action recognition, attaining a perfect accuracy of 100%. Our findings demonstrate the effectiveness of the superpixel diffractive neural network in simplifying the optical system and enhancing computational speed, opening new possibilities for real-time optical information processing applications.

# Water Body Detection Based On An Improved U-Net
Nowadays, deep learning has been widely used for water body detection because of its high precision databased water segmentation ability. Although the networks based on deep learning have shown higher automation, applicability and extraction accuracy than the traditional threshold methods in water body detection, only the translation equivariance of the convolution kernel is considered in these networks. Actually, for the detection of water body, its rotation equivariance also needs to be considered. For this goal, we here propose a new convolutional neural network by improving the U-Net with the rotation equivariant convolution and attention mechanism, which is simplified as GACNN. Experimental results on optical water body images demonstrate the effectiveness of the improved network based on U-net.
# An Information-Expanding Network for Water Body Extraction Based on U-Net
Water body extraction is an important issue in flood surveillance and environmental protection. With the development of neural network, deep learning has been widely used in the water body extraction task because of its powerful feature extraction ability. Even so, most of existing deep learning networks only take into account the translation equivariance of convolution kernel for water body extraction. Actually, in terms of water body, its orientations imaged by optical sensor are usually various. So, when the orientated images are not well contained in the training set, the networks may yield some unsatisfactory extraction results. To solve this problem, in this paper, we propose an information-expanding network IE-Unet based on the traditional network U-net, where the rotation equivariant convolution, rotation-based channel attention mechanism, and the optimized Batchnorm are adopted jointly. To quantitatively evaluate its edge extraction capability, a new edge index AOD is proposed as well. The experimental results on one public dataset of water body demonstrate the effectiveness of IE-Unet. Compared with the original U-net, the IOU value of IE-Unet is increased by 7$\%$, and the A0D value is reduced by 0.76.

# Large field of view Shack-Hartmann wavefront sensor based on high-density lens transfer function retrieval
The Shack-Hartmann wavefront sensor (SHWS) is known for its high accuracy and robust wavefront sensing capabilities. However, conventional compact SHWS confronts limitations in measuring field-of-view to meet emerging applications’ increasing demands. Here, we propose a high-density lens transfer function retrieval (HDLTR)-based SHWS to expand its field-of-view. In HDLTR-SHWS, an additional lens is introduced into the measurement system, which converges input wavefront with large aperture onto detectable aperture of sensor. A densely sampling set of phase delays is employed to retrieve the transfer function of the lens and to isolate lens distortion. Then, an accurate error compensation model is established to suppress errors introduced by the lens distortion, which is used to accurately demodulate the convergent wavefront. We also utilize a global spot matching method to reconstruct the converged wavefront with a large dynamic range. Our experimental results demonstrate that the HDLTR-SHWS expands the field-of-view of SHWS by a factor of 24.9 and achieves an accuracy of less than 1/80λ.

# PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation
Semantic segmentation is an important branch of image processing and computer vision. With the popularity of deep learning, various deep semantic segmentation networks have been proposed for pixel-level classification and segmentation tasks. However, the imaging angles are often arbitrary in real world, such as water body images in remote sensing, and capillary and polyp images in medical field, and we usually cannot obtain prior orientation information to guide these networks to extract more effective features. Additionally, learning the features of objects with multiple orientation information is also challenging, as most CNN-based semantic segmentation networks do not have rotation equivariance to resist the disturbance from orientation information. To address the same, in this paper, we first establish a universal convolution-group framework to more fully utilize the orientation information and make the networks rotation equivariant. Then, we  mathematically construct the padding-based rotation equivariant convolution mode (PreCM), which can be used not only for multi-scale images and convolution kernels, but also as a replacement component to replace multiple convolutions, like dilated convolution, transposed convolution, variable stride convolution. In order to verify the realization of rotation equivariance, a new evaluation metric named rotation difference (RD) is finally proposed. The experiments carried out on the datesets Satellite Images of Water Bodies, DRIVE and Floodnet show that the PreCM-based networks can achieve better segmentation performance than the original and data augmentation-based networks. In terms of the average RD value, the former is 0\% and the latter two are respectively 7.0503% and 3.2606%

# Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective of Manifold Distillation
Point cloud is often regarded as a discrete sampling of Riemannian manifold and plays a pivotal role in the 3D image interpretation. Particularly, rotation perturbation, an unexpected small change in rotation caused by various factors (like equipment offset, system instability, measurement errors and so on), can easily lead to the inferior results in point cloud learning tasks. However, classical point cloud learning methods are sensitive to rotation perturbation, and the existing networks with rotation robustness also have much room for improvements in terms of performance and noise tolerance. Given these, this paper remodels the point cloud from the perspective of manifold as well as designs a manifold distillation method to achieve the robustness of rotation perturbation without any coordinate transformation. In brief, during the training phase, we introduce a teacher network to learn the rotation robustness information and transfer this information to the student network through online distillation. In the inference phase, the student network directly utilizes the original 3D coordinate information to achieve the robustness of rotation perturbation. Experiments carried out on four different datasets verify the effectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN classification datasets with random rotation perturbations, our classification accuracy has respectively improved by 4.92% and 4.41%, compared to popular rotation-robust networks; on the ShapeNet and S3DIS segmentation datasets, compared to the rotation-robust networks, the improvements of mIoU are 7.36% and 4.82%, respectively. Besides, from the experimental results, the proposed algorithm also shows excellent performance in resisting noise and outliers. 

# Deep Learning–Based Facial and Skeletal Transformations for Surgical Planning
The increasing application of virtual surgical planning (VSP) in orthognathic surgery implies a critical need for accurate prediction of facial
and skeletal shapes. The craniofacial relationship in patients with dentofacial deformities is still not understood, and transformations
between facial and skeletal shapes remain a challenging task due to intricate anatomical structures and nonlinear relationships between
the facial soft tissue and bones. In this study, a novel bidirectional 3-dimensional (3D) deep learning framework, named P2P-ConvGC,
was developed and validated based on a large-scale data set for accurate subject-specific transformations between facial and skeletal
shapes. Specifically, the 2-stage point-sampling strategy was used to generate multiple nonoverlapping point subsets to represent highresolution
facial and skeletal shapes. Facial and skeletal point subsets were separately input into the prediction system to predict
the corresponding skeletal and facial point subsets via the skeletal prediction subnetwork and facial prediction subnetwork. For
quantitative evaluation, the accuracy was calculated with shape errors and landmark errors between the predicted skeleton or face with
corresponding ground truths. The shape error was calculated by comparing the predicted point sets with the ground truths, with P2PConvGC
outperforming existing state-of-the-art algorithms including P2P-Net, P2P-ASNL, and P2P-Conv. The total landmark errors
(Euclidean distances of craniomaxillofacial landmarks) of P2P-ConvGC in the upper skull, mandible, and facial soft tissues were 1.964 ±
0.904 mm, 2.398 ± 1.174 mm, and 2.226 ± 0.774 mm, respectively. Furthermore, the clinical feasibility of the bidirectional model was
validated using a clinical cohort. The result demonstrated its prediction ability with average surface deviation errors of 0.895 ± 0.175
mm for facial prediction and 0.906 ± 0.082 mm for skeletal prediction. To conclude, our proposed model achieved good performance
on the subject-specific prediction of facial and skeletal shapes and showed clinical application potential in postoperative facial prediction
and VSP for orthognathic surgery.

# Development and validation of a novel deep ensemble learning-based fully automatic multi-structure segmentation framework for craniomaxillofacial surgery
**Background**

Efficient and accurate segmentation of craniomaxillofacial (CMF) structures and individual teeth is a prerequisite and basis for advancing computer-assisted CMF
surgery. This study aimed to comprehensively benchmark the performance of three 3D U-Net-based deep learning models and develop a novel deep ensemble learningbased
fully automatic multi-structure segmentation model named CMF-ELSeg.

**Methods**

A total of ninety CMF CT scans were collected retrospectively from patients diagnosed with skeletal malocclusion during the process of combined orthodontic and
orthognathic surgical treatment. Ground truth segmentations were manually labeled by experienced experts. Fully automatic AI-based multi-structure segmentation algorithms were developed and validated. CMF-ELSeg was built on a coarse-to-fine cascaded segmentation network architecture and utilized an ensemble learning approach to combine the strengths of V-Net, nnU-Net, and 3D UX-Net. Model performance in segmentizing CMF structures and individual teeth was evaluated by comparing ground truth segmentations with model predictions using multiple metrics, including Dice score, F1-Score, and Intersection over Union (IoU).

**Results**

In the coarse segmentation task for the upper skull, mandible, cervical vertebra and pharyngeal cavity, both 3D UX-Net and nnU-Net achieved average dice scores
exceeding 0.96, average F1-score exceeding 0.98, and average IoU exceeding 0.93. For the fine segmentation and classification of individual teeth, the cascaded
segmentation network based on 3D UX-Net demonstrated optimal performance across all evaluation metrics, maintaining high accuracy and stability. Compared to individual models, CMF-ELSeg showed a 3%-5% improvement in Dice coefficients for facial soft tissue, upper skull, mandible bone, cervical vertebra, and pharyngeal cavity segmentation. Meanwhile, CMF-ELSeg consistently achieved high accuracy for individual teeth segmentation, with Dice coefficients exceeding 0.94 for most teeth.

**Conclusion**

CMF-ELSeg achieved high-precision segmentation of CMF structures and individual teeth by leveraging the diversity of multiple models, offering a practical tool for clinical practice and enhancing the efficacy of patient-specific treatment planning for CMF surgery.

# An Information-Expanding Network for Water Body Extraction Based on U-Net
Water body extraction is an important issue in flood surveillance and environmental protection. With the development of neural network, deep learning has been widely used in the water body extraction task because of its powerful feature extraction ability. Even so, most of existing deep learning networks only take into account the translation equivariance of convolution kernel for water body extraction. Actually, in terms of water body, its orientations imaged by optical sensor are usually various. So, when the orientated images are not well contained in the training set, the networks may yield some unsatisfactory extraction results. To solve this problem, in this paper, we propose an information-expanding network IE-Unet based on the traditional network U-net, where the rotation equivariant convolution, rotation-based channel attention mechanism, and the optimized Batchnorm are adopted jointly. To quantitatively evaluate its edge extraction capability, a new edge index AOD is proposed as well. The experimental results on one public dataset of water body demonstrate the effectiveness of IE-Unet. Compared with the original U-net, the IOU value of IE-Unet is increased by 7%, and the A0D value is reduced by 0.76.

